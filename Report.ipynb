{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93971d86a00532d9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "ΦΛΩΡΑ ΦΥΚΑ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36358837e936a722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:47:18.372331200Z",
     "start_time": "2025-04-06T17:47:06.100532400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985cf313b33fa0cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1.  Φόρτωση του CIFAR-100\n",
    "\n",
    "Το  dataset περιέχει 60.000 εικόνες σε 100 κατηγορίες. Κάθε εικόνα είναι σαν μονοδιάστατος πίνακας  32 x 32 pixels x 3ων  χρωμάτων RGB:\n",
    "- Τα πρώτα 1024 στοιχεία είναι οι τιμές του κόκκινου. \n",
    "- Τα επόμενα 1024 στοιχεία είναι οι τιμές του πράσινου. \n",
    "- Τα τελευταία 1024 στοιχεία είναι οι τιμές του μπλε.\n",
    "\n",
    "Δομή το αρχείου:\n",
    "-\tTrain -> 50.000 εικόνες για εκπαίδευση\n",
    "-\tTest -> 10.000 εικόνες για αξιολόγηση\n",
    "-\tMeta -> Πληροφορίες για τις κατηγορίες \n",
    "Στο link που παρέχεται στην εκφώνηση, δίνονται οδηγίες και ο κώδικας για την ανάγνωση του αρχείου. Τα παραπάνω αρχεία δίνομαι σε μορφή Pickle και διαβάζονται σε δυαδική μορφή.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a0c105de9c40f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:47:18.748827400Z",
     "start_time": "2025-04-06T17:47:18.394018800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([b'filenames', b'batch_label', b'fine_labels', b'coarse_labels', b'data'])\n"
     ]
    }
   ],
   "source": [
    "dataLoader = lib.DataLoader()\n",
    "train_data, test_data, meta_data = dataLoader.get_data()\n",
    "\n",
    "print(train_data.keys())  # Example: Checking available keys in train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d2510e0655e20",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Διαμόρφωση δεδομένων και ετικετών\n",
    "\n",
    "Εκτυπώνοντας τα δεδομένα των εικόνων παραπάνω, βρίσκουμε και τo ‘fine_labels’ για τις κατηγορίες των εικόνων:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7bc4339bf0c9f4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:47:18.750339Z",
     "start_time": "2025-04-06T17:47:18.735034400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = train_data[b'data']  # Τα δεδομένα των εικόνων\n",
    "y_train = np.array(train_data[b'fine_labels']) # Οι κατηγορίες των εικόνων"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d476fe28786c4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Με παρόμοια λογική κινούμαστε και για τα δεδομένα αξιολόγησης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af6732ad0b2590cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:47:18.789598700Z",
     "start_time": "2025-04-06T17:47:18.749333300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = train_data[b'data']\n",
    "y_test = np.array(train_data[b'fine_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec459519ef96b5c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Για τα δεδομένα των εικόνων:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f478194cc4ecd328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:47:18.938611200Z",
     "start_time": "2025-04-06T17:47:18.764450700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)  # (samples, height, width, channels)\n",
    "X_test = X_test.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a260bbb5b8596a1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Με το -1 στο reshape βρίσκει αυτόματα τον αριθμό  [NumPy Documentation](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html)\n",
    "- Η χρήση του transpose είναι για την αναδιαμόρφωση των δεδομένων ώστε η τελική μορφή να είναι ( αριθμός εικόνων, 32, 32, 3 ) και να εξυπηρετεί καλύτερα. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835d62ac9a1539d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:47:21.948563Z",
     "start_time": "2025-04-06T17:47:18.773525200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (50000, 32, 32, 3)\n",
      "Training labels shape: 50000\n",
      "Test data shape: (50000, 32, 32, 3)\n",
      "Test labels shape: 50000\n"
     ]
    }
   ],
   "source": [
    "lib.displayDataTest(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c8824c434af4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Επιλογή 10 κλάσεων"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec40aef34239ba77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:47:21.949572700Z",
     "start_time": "2025-04-06T17:47:21.914353200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_names = dataLoader.get_names()\n",
    "#print(label_names) \n",
    "selected_names = [\"orange\", \"snake\", \"apple\", \"turtle\", \"bee\", \"bicycle\", \"keyboard\", \"pear\", \"plate\", \"telephone\"]\n",
    "#selected_names = lib.pick_random_classes(label_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370f85aa71db5f1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3.  Η εφαρμογή θα γίνει στις κλάσεις που επιλέχθηκαν τυχαία παραπάνω\n",
    "\n",
    "Αντιστοίχιση των επιλεγμένων κλάσεων στα ονόματα των κλάσεων για να βρω τα αντίστοιχα indices, τα οποία αντιστοιχούν στις κλάσεις που θέλω να κρατήσω μέσα στα δεδομένα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e52a0c04f80db82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:47:21.950943600Z",
     "start_time": "2025-04-06T17:47:21.918044800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_filtered, y_train_filtered, X_test_filtered, y_test_filtered = lib.filter_data_by_classes(\n",
    "    X_train, y_train, X_test, y_test, selected_names, label_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73675af1fef31e6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Tαξινομητής 3-NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df5d601b09ede2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Οι τιμές για τις εικόνες είναι μεταξύ ( 0-255 ). Διαιρώντας με το 255 οι τιμές θα κυμένονται απο 0-1. Για καλύτερο αποτέλεσμα τα δεδομένα είναι καλύτερο να είναι σε μια κοινή κλίματα. Η κανονικοποίηση βοηθάει και στην αποφυγή κυριαρχίας μεγάλων τιμών στην απόσταση.\n",
    "\n",
    "Επίσης για να λειτουργήσει το k-NN τα δεδομένα είναι σε μορφή 1D παράσταση, (3072,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f7afe74ba97e251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:47:23.484404900Z",
     "start_time": "2025-04-06T17:47:21.950943600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "X_train_filtered, X_test_filtered = lib.normalize_and_reshape_numpy(X_train_filtered, X_test_filtered)\n",
    "\n",
    "accuracy_knn = lib.train_and_evaluate_knn(X_train_filtered, y_train_filtered, X_test_filtered, y_test_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35b2f57eba5f34",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. Autoencoder\n",
    "\n",
    "Ο Autoencoder είναι ένα νευρωνικό δίκτυο αποτελούμενο απο δύο μέρη:\n",
    "\n",
    "Encoder -> θα μειώσει τη διάσταση των δεδομένων\n",
    "Αποτελείται από Conv2D τα οποία συμπιέζουν την εικόνα σε διαστάσεις μικρότερες σε κάθε στρώμα. Σε κάθε επίπεδο μετατρέπει σε απλούστερες αναπαραστάσεις, πιάνοντας έτσι τα πιο σημαντικά χαρακτηριστικά της εικόνας επειδή μειώνει την πληροφορία που είναι λιγότερο σημαντική. Στο τέλος τα δεδομένα μετατρέπονται σε διάσταση 64, η οποία και είναι η συμπιεσμένη αναπαράσταση της εικόνας.\n",
    "\n",
    "Decoder -> θα επαναφέρει τις αρχικές εικόνες\n",
    "Έχουμε μια χαμηλής διάσταση αναπαράσταση των εικόνων που θα χρησιμοποιηθεί για ταξινόμηση με k-ΝΝ. \n",
    "\n",
    "Ο Autoencoder εκπαιδεύεται με loss function Mean Squared Error, υπολογίζοντας τη διαφορά μεταξύ της εικόνας που κατασκευάστηκε και της αρχικής εικόνας, ενώ ο αλγόριθμος Adam χρησιμοποιείται για τη βελτιστοποίηση των παραμέτρων του δικτύου. Το μοντέλο εκπαιδεύεται για 20 epochs με batch size 128.\n",
    "\n",
    "Με τον εκπαιδευμένο autoencoder, και τον encoder εξάγουμε τις κωδικοποιημένες αναπαραστάσεις των εικόνων τις οποίες χρησιμοποιύμε ώς είσοδο για τον ταξινομητή κ-ΝΝ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56443db9c9b81eac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Μέχρι στιγμής τα δεδομένα ήταν σε μορφή numpy representation (3072). Τώρα η μορφή των δεδομένων αλλάζει ώστε να είναι όπως χρειάζεται για το PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "518a221754fd8d6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:47:41.723964Z",
     "start_time": "2025-04-06T17:47:23.485404300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0536\n",
      "Epoch 2/20, Loss: 0.0585\n",
      "Epoch 3/20, Loss: 0.0597\n",
      "Epoch 4/20, Loss: 0.0692\n",
      "Epoch 5/20, Loss: 0.0401\n",
      "Epoch 6/20, Loss: 0.0314\n",
      "Epoch 7/20, Loss: 0.0392\n",
      "Epoch 8/20, Loss: 0.0269\n",
      "Epoch 9/20, Loss: 0.0263\n",
      "Epoch 10/20, Loss: 0.0249\n",
      "Epoch 11/20, Loss: 0.0266\n",
      "Epoch 12/20, Loss: 0.0207\n",
      "Epoch 13/20, Loss: 0.0263\n",
      "Epoch 14/20, Loss: 0.0177\n",
      "Epoch 15/20, Loss: 0.0204\n",
      "Epoch 16/20, Loss: 0.0174\n",
      "Epoch 17/20, Loss: 0.0217\n",
      "Epoch 18/20, Loss: 0.0213\n",
      "Epoch 19/20, Loss: 0.0163\n",
      "Epoch 20/20, Loss: 0.0226\n",
      "Accuracy: 0.69\n"
     ]
    }
   ],
   "source": [
    "train_loader, X_train_tensor, X_test_tensor = lib.prepare_data_for_pytorch(X_train_filtered, X_test_filtered)\n",
    "\n",
    "autoencoder = lib.Autoencoder(encoded_dim=64)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "lib.train_autoencoder(autoencoder, train_loader, optimizer, criterion, epochs=20)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_train_encoded = autoencoder.encoder(X_train_tensor).cpu().numpy()\n",
    "    X_test_encoded = autoencoder.encoder(X_test_tensor).cpu().numpy()\n",
    "\n",
    "autoencoder_knn = lib.train_and_evaluate_knn(X_train_encoded, y_train_filtered, X_test_encoded, y_test_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4987f2230abf9a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5. Comparizon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46defa6d0f530304",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Δεδομένου ότι έχουμε ένα classification task για τη σύγκριση θα δοθεί περισσότερο βάση στο metric accuracy:\n",
    "- είναι βασικό μέτρο αξιολόγησης για προβλήματα ταξινόμησης\n",
    "- έχουμε 10 κλάσεις και θέλουμε να δούμε αν ταξινομούνται σωστά\n",
    "- χρησιμοποιούμε τον ίδιο ταξινομητή k-NN σε δύο διαφορετικούς χώρους έτσι μας επιτρέπει να συγκρίνουμε αντικειμενικά raw pixels vs autoencoder embeddings\n",
    "- αν ο Autoencoder πετυχαίνει υψηλότερο accuracy, σημαίνει ότι έχει μάθει καλύτερη αναπαράσταση των δεδομένων.\n",
    "\n",
    "Θα μπορούσε να γίνει σύγκριση και με άλλα metrics όπως precision, recall, f1-score αλλά όπως φαίνεται και παρακάτω, έχουμε ένα ισορροπημένο dataset με ομοιόμορφη κατανομή στις κλάσεις, οπότε ίσως και να μην είναι τόσο χρήσιμα σε αυτή την περίπτωση."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5159c246560b63de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:47:41.841202200Z",
     "start_time": "2025-04-06T17:47:41.716449700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 500 training samples\n",
      "Class 86: 500 training samples\n",
      "Class 39: 500 training samples\n",
      "Class 8: 500 training samples\n",
      "Class 78: 500 training samples\n",
      "Class 6: 500 training samples\n",
      "Class 53: 500 training samples\n",
      "Class 93: 500 training samples\n",
      "Class 61: 500 training samples\n",
      "Class 57: 500 training samples\n"
     ]
    }
   ],
   "source": [
    "train_data_by_class = lib.group_data_by_class(X_train_filtered, y_train_filtered)\n",
    "test_data_by_class = lib.group_data_by_class(X_test_filtered, y_test_filtered)\n",
    "\n",
    "for class_label, samples in train_data_by_class.items():\n",
    "    print(f\"Class {class_label}: {samples.shape[0]} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d06671d0bf6cd95",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Autoencoder vs 3-ΝΝ ταξινομητή στον αρχικό χώρο\n",
    "\n",
    "Στην αρχή, στα δεδομένα των εικόνων έγινε ένα reshape και μια κανονικοποίηση. Επομένως, θεωρούμε ότι βρίσκονται στον αρχικό χώρο, original feature space.\n",
    "\n",
    "Το accuracy για το Αutencoder είναι 0.68, όπως φαίνεται σε προηγούμενο αποτέλεσμα. \n",
    "\n",
    "α) 3-NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc01a5e14280ae46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T17:47:42.562376100Z",
     "start_time": "2025-04-06T17:47:41.838202200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "accuracy_original_knn = lib.train_and_evaluate_knn(X_train_filtered, y_train_filtered, X_test_filtered, y_test_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d351d1fae059d00",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Το μοντέλο 3-NN έχει λίγο καλύτερη ακρίβεια από το Autoencoder + k-NN.\n",
    "Ο Autoencoder συμπιέζει και ενδεχομένως χάνει κάποιες λεπτομέρειες που είναι σημαντικές για την ταξινόμηση.\n",
    "\n",
    "Ωστόσο, η διαφορά 0.02 είναι πολύ μικρή, κάτι που σημαίνει ότι ο Autoencoder έχει μάθει κάποια χρήσιμα χαρακτηριστικά των δεδομένων και μπορεί να επιτύχει παρόμοια αποτελέσματα με το k-NN.\n",
    "\n",
    "Συμπέρασμα: ο Autoencoder μπορεί να βοηθήσει στην εξαγωγή χρήσιμων χαρακτηριστικών αλλά δεν είναι πάντα καλύτερο απο το απλό k-NN.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc154f4125c38316",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "β) CLIP\n",
    "\n",
    "Στο paper \"Learning Transferable Visual Models From Natural Language Supervision\" υπάρχει link για το επίσημο repository για το CLIP. Ακολουθώντας τις οδηγίες εγκατέστησα τα απαραίτητα πακέτα εκτός από το cuda toolkit καθώς επέλεξα να χρησιμοποιήσω τη cpu αντί τη gpu. \n",
    "\n",
    "Πολύ ενδιαφέρον έχει ότι το CLIP είναι ένα προεκπαιδευμένο μοντέλο που αντιστοιχίζει εικόνες με κάποια περιγραφή, όπως λέξη, ετικέτα κλπ σε φυσική γλώσσα όπως στη δική μας περίπτωση και ταξινομεί χωρίς επιπλέον εκπαίδευση πάνω σε συγκεκριμένες κατηγορίες, zero-shot classification όπως αναφέρεται και στο paper.\n",
    "\n",
    "Στη δική μου υλοποίηση:\n",
    "\n",
    "- Χρησιμοποίησα το μοντέλο ViT-B/32 από το clip repository\n",
    "- Για τα test data έκανα ένα reshape σε ( samples, height, width, channels ) όπως τα χρειάζεται το μοντέλο CLIP.\n",
    "- Για κάθε όνομα των κατηγοριών που επέλεξα έφτιαξα με το encode_text προτάσεις με τη συγκεκριμένη μορφή που το CLIP χρησιμοποιεί. Προτάσεις όπως \"a photo of a X\".\n",
    "- Η επιλογή κατηγορίας γίνεται με βάση το εσωτερικό γινόμενο μεταξύ εικόνας και text embeddings. Γίνεται δηλαδή δοκιμή κάθε εικόνας με τις περιγραφές και κρατάμε αυτή που της ταιριάζει περισσότερο. \n",
    "- Τέλος γίνεται υπολογισμός τησ ακρίβειας πάνω στο test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b220dc860e19ce87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T18:07:31.547249500Z",
     "start_time": "2025-04-06T17:47:42.553368400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Zero-Shot Predict: 100%|██████████| 5000/5000 [19:48<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot CLIP Accuracy: 0.9014\n"
     ]
    }
   ],
   "source": [
    "y_test_filtered_names = [label_names[i] for i in y_test_filtered]\n",
    "acc_clip = lib.zero_shot_clip_predict(X_test_filtered, y_test_filtered_names, selected_names)\n",
    "print(f\"Zero-Shot CLIP Accuracy: {acc_clip:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d913984591efd2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Το μοντέλο CLIP πεετυχαίνει καλύτερα αποτελέσματα από τα k-NN και Autoencoder + k-NN. H ακρίβεια του φτάνει το 90%. Σε σύγκριση με περίπου 70% που έχει το μοντέλο 3-NN είναι περίπου 20% καλύτερο στην ταξινόμηση των εικόνων στις 10 επιλεγμένες κατηγορίες.\n",
    "\n",
    "Το πιο εντυπωσιακό είναι ότι το μοντέλο δεν έχει εκπαιδευτεί πάνω σε αυτό το dataset, ούτε έγινε κάποια προσαρμογή για τη δική μου χρήση. Το μόνο που έκανα ήταν να του δώσω κάποιες προτάσεις ως ετικέτες και τις εικόνες.\n",
    "\n",
    "Συμπέρασμα: Το μοντέλο CLIP είναι πολύ καλό στην αναγνώριση εικόνων, τουλάχιστον με βάση τις εικόνες που του έδωσα στο δικό μου δείγμα."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ac026d184fc298",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "γ) Αλλο εκπαιδευμένο embedding (off-the-shelf):  ResNet-50\n",
    "\n",
    "Για το embedding που επέλεξα, χρησιμοποίησα το μοντέλο ResNet-50 επειδή έχει εκπαιδευτεί στο dataset ImageNet. Αυτό το μοντέλο θα έπρεπε να είναι κατάλληλο για ταξινόμηση τύπου zero-shot, αλλά ενδέχεται να μην είναι τόσο ακριβές όσο το μοντέλο CLIP, τουλάχιστον όχι χωρίς fine-tuning. \n",
    "\n",
    "Στη δική μου υλοποίηση:\n",
    "\n",
    "- Χρησιμοποίησα την προεκπαιδευμένη έκδοση που παρέχεται από το TorchVision.\n",
    "- Για τα test data έκανα ένα reshape σε (samples, height, width, channels) όπως χρειάζεται το μοντέλο ResNet-50. \n",
    "- Έκανα τις απαραίτητες μετατροπές προεπεξεργασίας ώστε η εικόνα να είναι στη μορφή που χρειάζεται το ResNet-50. \n",
    "- Αυτή τη φορά δεν χρειάζονται prompts, όπως έγινε με το μοντέλο CLIP. Παίρνουμε την πρόβλεψη από το ResNet και τη χρησιμοποιούμε ως δείκτη για να εντοπίσουμε την αντίστοιχη κατηγορία.\n",
    "- Τέλος γίνεται υπολογισμός της ακρίβειας πάνω στο test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16d8c214",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T18:14:37.299665700Z",
     "start_time": "2025-04-06T18:07:31.406299300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ResNet-50 Prediction: 100%|██████████| 5000/5000 [07:05<00:00, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot ResNet50 Accuracy: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc_res = lib.resnet_predict(X_test_filtered, y_test_filtered_names, selected_names)\n",
    "print(f\"Zero-Shot ResNet50 Accuracy: {acc_res:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2384fcc19df3ec1f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Το μοντέλο ResNet-50 δε φαίνεται να πετυχαίνει το επιθυμητό αποτέλεσμα όταν χρησιμοποιείται ως προεκπαιδευμένο. Έχει μόνο 10% ακρίβεια, κάτι που το κάνει το μοντέλο μ ετη χαμηλότερη απόδοση, από αυτά που συγκρίθηκαν.\n",
    "\n",
    "Δεν είναι σαφές γιατί συμβαίνει αυτό, καθώς η υλοποίηση ακολουθεί το ίδιο μοτίβο με αυτό του μοντέλου CLIP. Ένας πιθανός λόγος θα μπορούσε να είναι ότι το dataset στο οποίο εκπαιδεύτηκε το ResNet-50, δηλαδή το ImageNet, ίσως δεν έχει τόση ποικιλία στις ετικέτες όσο θα περιμέναμε για το dataset που χρησιμοποιήθηκε σε αυτή την άσκηση.\n",
    "\n",
    "Επιπλέον, το μοντέλο δεν έγινε fine-tuned, κάτι που μπορεί να επηρέασε τα αποτελέσματα.\n",
    "\n",
    "Συμπέρασμα: Το ResNet-50 τουλάχιστον όπως εφαρμόστηκε εδώ, δεν φαίνεται να είναι η κατάλληλη επιλογή για μοντέλο ταξινόμησης εικόνων τύπου zero-shot στο dataset CIFAR-100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188a8c66436477a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6. Επιλογή τυχαίων κλάσεων και τελική σύγκριση\n",
    "\n",
    "Η παραπάνω ανάλυση έγινε έχοντας επιλέξει συγκεκριμένες κλάσεις. Αν και η επιλογή ήταν εν μέρει τυχαία, είχα στόχο να διευκολύνω τα μοντέλα επιλέγοντας οντότητες με έντονες διαφορές πχ. δεν συνδύασα παρόμοιες κατηγορίες όπως σκύλος-λύκος, αλλά αντίθετες όπως φίδι-πορτοκάλι.\n",
    "\n",
    "Για να μια πιο γενική εικόνα της απόδοσης, επανέλαβα την ίδια ανάλυση με πλήρως τυχαία δειγματοληψία από τις κλάσεις του dataset.\n",
    "Παρακάτω είναι τα αποτελέσματα: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e618cef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T18:14:37.314744900Z",
     "start_time": "2025-04-06T18:14:37.129509500Z"
    }
   },
   "outputs": [],
   "source": [
    "#lib.print_model_performance(selected_names, accuracy_knn, autoencoder_knn, accuracy_original_knn, acc_clip, acc_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbad3a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d1b8414",
   "metadata": {},
   "source": [
    "### Επιλεγμένες κλάσεις\n",
    "`orange`, `snake`, `apple`, `turtle`, `bee`, `bicycle`, `keyboard`, `pear`, `plate`, `telephone`\n",
    "\n",
    "| Model              | Accuracy |\n",
    "|--------------------|----------|\n",
    "| KNN                | 0.6990   |\n",
    "| Autoencoder KNN    | 0.6768   |\n",
    "| KNN (Original)     | 0.6990   |\n",
    "| CLIP               | 0.9008   |\n",
    "| ResNet-50          | 0.1308   |\n",
    "\n",
    "\n",
    "### Τυχαία Επιλεγμένες κλάσεις\n",
    "\n",
    "1: `baby`, `bear`, `beaver`, `mountain`, `pine_tree`, `beetle`, `skyscraper`, `chimpanzee`, `wolf`, `sea`\n",
    "\n",
    "| Model              | Accuracy |\n",
    "|--------------------|----------|\n",
    "| KNN                | 0.6562   |\n",
    "| Autoencoder KNN    | 0.6726   |\n",
    "| KNN (Original)     | 0.6562   |\n",
    "| CLIP               | 0.8714   |\n",
    "| ResNet-50          | 0.0892   |\n",
    "\n",
    "\n",
    "2: `ray`, `mouse`, `hamster`, `beetle`, `cup`, `telephone`, `trout`, `rocket`, `palm_tree`, `chair`\n",
    "\n",
    "| Model              | Accuracy |\n",
    "|--------------------|----------|\n",
    "| KNN                | 0.7000   |\n",
    "| Autoencoder KNN    | 0.6368   |\n",
    "| KNN (Original)     | 0.7000   |\n",
    "| CLIP               | 0.8556   |\n",
    "| ResNet-50          | 0.0434   |\n",
    "\n",
    "\n",
    "3: `mouse`, `trout`, `chimpanzee`, `chair`, `motorcycle`, `spider`, `willow_tree`, `castle`, `streetcar`, `worm`\n",
    "\n",
    "| Model              | Accuracy |\n",
    "|--------------------|----------|\n",
    "| KNN                | 0.6770   |\n",
    "| Autoencoder KNN    | 0.6712   |\n",
    "| KNN (Original)     | 0.6770   |\n",
    "| CLIP               | 0.9148   |\n",
    "| ResNet-50          | 0.0856   |\n",
    "\n",
    "Ανεξάρτητα από το ποιές κλάσεις επιλέχθηκαν, το CLIP αποδίδει σταθερά καλύτερα έναντι των άλλων μοντέλων. Διατηρεί πολυ υψηλό accuracy πάνω απο 85% ακόμα και με τυχαία δειγματοληψία. \n",
    "\n",
    "Τα παραδοσιακά μοντέλα k-NN και Αutoencoder + k-NN, έχουν σταθερό αλλά χαμηλό accuracy, περίπου 65-70%. Αξιοπρόσεκτο είναι ότι ο Autoencoder δε φαίνεται να βελτιώνει σημαντικά την απόδοση του απλού k-NN, σε κάποιες περιπτώσεις είναι ελαφρώς χειρότερο. \n",
    "\n",
    "Το ResNet-50 είχε πολύ χαμηλή ακρίβεια, που μάλλον σημαίνει ότι δεν έχει προσαρμοστεί καλά στο συγκεκριμένο σύνολο δεδομένων ή απλώς δε λειτουργεί καλά με τον τρόπο που χρησιμοποιήθηκε εδώ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a61bc37364917",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
